{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine\n",
    "\n",
    "## Theoretical overview\n",
    "\n",
    "Support vector machines are supervised learning alghorithms that can be used both for classification or regression applications.\n",
    "Here I will focus on their implementations for classification problems, such as the one addressed in this project, briefly explaining the foundamentals of this method (for further details see Géron, 2019).\n",
    "\n",
    "A support vector machine works constructing, in the feature's space, a set of hyperplanes working as decision boundaries between different classes. The choice of the proper hyperplanes is carried out maximizing their distance to the nearest training data points of any class which are called support vectors.\n",
    "\n",
    "The following image shows an example of application in a 2-dimensional feature's space (from _Support Vector Machines — scikit-learn 1.0.2 documentation_, n.d.). The dashed lines are called \"decision margins\" on which lie the support vectors. The SVM method consists in maximizing the area between these margins. \n",
    "\n",
    "<center>\n",
    "<img src=\"images/svm1.png\" width=\"50%\">\n",
    "</center>\n",
    "\n",
    "Considering a binary classification problem a SVM can be trained on the dataset of N elements\n",
    "$$\n",
    " D = \\{(y_i, \\mathbf{x_i}):\\hspace{7pt} \\mathbf{x_i}\\in \\mathbb{R}^p;\\hspace{7pt} y_i\\in \\{-1, 1\\};\\hspace{7pt} i=1,...,N\\}\n",
    "$$\n",
    "solving the problem\n",
    "$$\n",
    "\\min_{\\mathbf{\\beta}, \\beta_0, \\xi}\\left( \\frac12 \\mathbf{\\beta}^T \\mathbf{\\beta} + C \\sum_{i=1}^N \\xi_i\\right)\n",
    "$$\n",
    "subject to the conditions\n",
    "$$\n",
    "    y_i(\\mathbf{x_i}^T\\mathbf{\\beta} + \\beta_0) \\geq 1 - \\xi_i, \\hspace{7pt} \\forall i \\\\\n",
    "    \\xi_i \\geq 0, \\hspace{10pt} \\forall i\n",
    "$$\n",
    "\n",
    "In these equations, the hyperplane that acts as a decision boundary is determined by the vector $\\mathbf{\\beta}\\in \\mathbb R^p$ and $\\beta_0$ through the equation $\\mathbf{x_i}^T\\mathbf{\\beta} + \\beta_0 = 0$ and defines the decision function  $d_i = \\mathbf{x_i}^T\\mathbf{\\beta} + \\beta_0$. The first group of conditions require that each point in the training dataset is classified correctly thrugh the rule:\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\hat y_i = 1, \\hspace{7pt} \\text{if} \\hspace{5pt} d_i\\geq 1 \\\\\n",
    "\\hat y_i = -1 \\hspace{7pt} \\text{if} \\hspace{5pt} d_i\\leq -1\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Note that the boundaries for $d_i$ are set to $\\pm 1$ but the functional margin can be arbitrarly wide since the norm of $\\mathbf{\\beta}$ can vary freely. The optimization problem addresses indeed this problem requiring the norm of $\\mathbf{\\beta}$ to be minimum which is equivalent to maximizing the area between the decision margins.\n",
    "For each element $i$ in the dataset, the additional parameter $\\xi_i$ is also introduced. It measures the distance of the point $i$ from its correct decision margin allowing some of the points to cross it.\n",
    "The second addend, of the optimized function, requires these parameters to be minimum introducing a hyperparameter $C$ that controls the strength of this penalty. Allowing data points to cross the margins helps training more flexible models reducing overfitting and generalization errors.\n",
    "\n",
    "This optimization problem can be solved using Karush–Kuhn–Tucker (KKT) conditions, which lead to the dual problem \n",
    "$$\n",
    "\\min_{\\alpha_i}\\left( \\sum_{i=1}^N \\alpha_i -\\frac12 \\sum_{i=1\\\\i'=1}^N \\alpha_i\\alpha_{i'}y_iy_{i'}\\mathbf{x_i}^T\\mathbf{x_{i'}}\\right)\n",
    "$$\n",
    "subject to\n",
    "$$\n",
    "    \\alpha_i \\geq 0 \\hspace{5pt} \\forall i\n",
    "$$\n",
    "From the $\\alpha_i$ values is then possible to compute $\\beta$ and $\\beta_0$ obtaining the solution of the original problem.\n",
    "\n",
    "The main limitation of this method as so far presented is the linear nature of boundaries between classes in the features's space. Some problems are not linearly separable. To overcome this limitation the feature space can be expanded mapping each data point in a new point whose coordinates are determined applying some functions to the original features.\n",
    "This means mapping, for example, $\\mathbf{x_i} \\in \\mathbb R^p$ to\n",
    "$$\n",
    "    \\mathbf{h}\\left[\\left(\\begin{array}{c}\n",
    "    x_1\\\\x_2\\\\x_3\n",
    "    \\end{array}\n",
    "    \\right)\\right] =  \\left(\n",
    "    \\begin{array}{c}\n",
    "x_1 \\\\ x_2 \\\\ x_3  \\\\ x_1^2 \\\\ x_2^2 \\\\ x_3^2\n",
    "    \\end{array} \\right)\n",
    "$$ \n",
    "This tecniques does not modify the functioning of the algorithm which still works in the same way on the new features set. However, the complexity of the optimization problem is increased significantly reducing performances and thus requiring larger computational resources.\n",
    "In order to control the complexity of the optimization problem, the kernel method is used: in the dual optimization problem previously presented it is clear that the feature vectors $\\mathbf{x}_i$, which are then substituted by $\\mathbf{h}(\\mathbf{x_i})$, appear in the function to minimize, only through their dot product $K(\\mathbf{h_i}, \\mathbf{h_j})$.\n",
    "Therefore, only the kernel function K is chosen while the mapping function $\\mathbf{h}(\\mathbf{x})$ in the higher space is implicitly set by the kernel choice. This method is computationally cheaper.\n",
    "\n",
    "For multi class classification, the library used in this project, `sklearn.svm.SVC`, implements the \"one-versus-one\" approach: a different classifier is constructed for each pair of classes (_sklearn.svm.SVC — scikit-learn 1.0.2 documentation_, n.d.). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "In the mathematical formulation presented above there are two hyperparameters that must be set during the definition of the model: the constant C and the Kernel.\n",
    "\n",
    "### The regularization parameter (C)\n",
    "This constant determines how much penalty should be assigned to data points crossing the decision margins. A higher value of this constant correspond to harder conditions on the margins meaning that their crossing is less tolerated. On the other end, lower values allow softer conditions with opposite implications.\n",
    "One of the main downsides of hard conditions, i.e. high C, is the risk of overfitting the data leading to large generalization errors when the model is tested on new elements.\n",
    "A fine tuning of this hyperparameter can help avoid this situation and achieve more flexible models.\n",
    "In this notebook six values will be tried for each kernel: 0.1, 0.5, 1, 5, 10, 100.\n",
    "\n",
    "### The Kernel\n",
    "\n",
    "A kernel is a function of two vectors $\\mathbf{x}$ and $\\mathbf{x'}$ that computes the dot product of $\\mathbf{\\phi(x)}$ and $\\mathbf{\\phi(x')}$ for a certain map $\\mathbf{\\phi(\\cdot)}$, without the need to explicitly compute this map.\n",
    "In support vector machines they are used, as discussed in the previous section, to efficiently solve the dual optimization problem mapping, with $\\phi$, the feature's space to a larger one to overcome the linear nature of the method in problems not solvable with linear decision boundaries.\n",
    "In this notebook I will try 3 different kernels which are presented below. The default values of their hyperparameters are defined in _sklearn.svm.SVC_.\n",
    "\n",
    "#### - Linear\n",
    "$$\n",
    "K(\\mathbf x, \\mathbf{x'}) = \\mathbf x^T \\mathbf{x'}\n",
    "$$\n",
    "with this kernel the original features remain unchanged and standard linear margins are fitted by the method.\n",
    "\n",
    "#### - Polynomial\n",
    "$$\n",
    "K(\\mathbf x, \\mathbf{x'}) = (\\gamma \\mathbf x^T \\mathbf{x'})^d\n",
    "$$\n",
    "This kernel corresponds to a polynomial map of the original features. It is characterized by two hyperparameters: $d$ controls the degree of the polynomial map, the default value used in the following implementation is 3;\n",
    "$\\gamma$ is a hyperparameter used to rescale the feature's product whose the default value used here is `1/(n_features*X.var())`.\n",
    "\n",
    "#### - Radial Basis Function (rbf)\n",
    "$$\n",
    "K(\\mathbf x, \\mathbf{x'}) = \\text{exp}(-\\gamma \\Vert \\mathbf x - \\mathbf{x'}\\Vert^2)\n",
    "$$\n",
    "This kernel is associated to a map $\\phi$ of the original feature's space to an infinite dimensional space. The $\\gamma$ hyperparameter has the same role of the one in the previous kernel with also the same default value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import my_utils as u\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22348/22348 [00:24<00:00, 925.06it/s] \n"
     ]
    }
   ],
   "source": [
    "#retrieve the training dataset\n",
    "x_train, y_train, class_labels = u.get_images(u.train_files)\n",
    "\n",
    "#SVC requires an input array of shape (n_samples, n_features)\n",
    "x_train = np.reshape(x_train, newshape=(x_train.shape[0], -1))\n",
    "x_train = x_train/256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22348, 12780)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "#I am going to try different models: C=0.1,0.5,1,5,10,100\n",
    "#for linear, rbf and poly kernels\n",
    "#manually implementing grid search\n",
    "\n",
    "#defining hyperparameters to try\n",
    "c_totry = [0.1,0.5,1,5,10,100]\n",
    "kernels_totry = ['linear','rbf', 'poly']\n",
    "\n",
    "#starting training\n",
    "for k in kernels_totry:\n",
    "    for c in c_totry:\n",
    "        tstart = timer()\n",
    "        classifier=SVC(C=c, kernel=k)\n",
    "        classifier.fit(x_train, y_train)\n",
    "        tend = timer()\n",
    "        with open(f\"{k}svc_{c}.model\", 'wb') as fmod:\n",
    "            pickle.dump(classifier, fmod)\n",
    "        with open(f\"run_times.txt\", \"a\") as ftimes:\n",
    "            ftimes.write(f\"{k}svc_{c}.model,\\t{tend-tstart}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "I will use different methods to compare the results of the 18 models trained in the previous section. All of these methods rely on the confusion matrix which is defined such as a matrix whose component $C_{i,j}$ is equal to the number of observations known to be in group $i$ and predicted to be in group $j$.\n",
    "\n",
    "The following cells compute and show the confusion matrices obtained applying each of the trained models to the test set of the gravity spy dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4720/4720 [00:03<00:00, 1205.91it/s]\n"
     ]
    }
   ],
   "source": [
    "#loading test set and reshaping it in the correct shape\n",
    "x_test, y_test, test_data = u.get_images(u.test_files)\n",
    "x_test = np.reshape(x_test, newshape=(x_test.shape[0], -1))\n",
    "x_test = x_test/256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opening model linearsvc_0.1.model\n",
      "opening model linearsvc_0.5.model\n",
      "opening model linearsvc_1.model\n",
      "opening model linearsvc_5.model\n",
      "opening model linearsvc_10.model\n",
      "opening model linearsvc_100.model\n",
      "opening model rbfsvc_0.1.model\n",
      "opening model rbfsvc_0.5.model\n",
      "opening model rbfsvc_1.model\n",
      "opening model rbfsvc_5.model\n",
      "opening model rbfsvc_10.model\n",
      "opening model rbfsvc_100.model\n",
      "opening model polysvc_0.1.model\n"
     ]
    }
   ],
   "source": [
    "#loading all the models\n",
    "kernels = ['linear', 'rbf', 'poly']\n",
    "c_totry = [0.1,0.5,1,5,10,100]\n",
    "results_df = pd.DataFrame()\n",
    "for k in kernels:\n",
    "    for c in c_totry:\n",
    "        filename = f\"{k}svc_{c}.model\"\n",
    "        print(f'opening model {filename}')\n",
    "        with open(filename, 'rb') as f:\n",
    "            classifier = pickle.load(f)\n",
    "\n",
    "        new_entry = {'model_file': filename, 'kernel': k, 'c': c, 'model_label': f'{k} (c={c})', 'classifier': classifier}\n",
    "        results_df = results_df.append(new_entry, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are shown all the trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c</th>\n",
       "      <th>classifier</th>\n",
       "      <th>kernel</th>\n",
       "      <th>model_file</th>\n",
       "      <th>model_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>SVC(C=0.1, kernel='linear')</td>\n",
       "      <td>linear</td>\n",
       "      <td>linearsvc_0.1.model</td>\n",
       "      <td>linear (c=0.1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.5</td>\n",
       "      <td>SVC(C=0.5, kernel='linear')</td>\n",
       "      <td>linear</td>\n",
       "      <td>linearsvc_0.5.model</td>\n",
       "      <td>linear (c=0.5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>SVC(C=1, kernel='linear')</td>\n",
       "      <td>linear</td>\n",
       "      <td>linearsvc_1.model</td>\n",
       "      <td>linear (c=1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>SVC(C=5, kernel='linear')</td>\n",
       "      <td>linear</td>\n",
       "      <td>linearsvc_5.model</td>\n",
       "      <td>linear (c=5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.0</td>\n",
       "      <td>SVC(C=10, kernel='linear')</td>\n",
       "      <td>linear</td>\n",
       "      <td>linearsvc_10.model</td>\n",
       "      <td>linear (c=10)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>100.0</td>\n",
       "      <td>SVC(C=100, kernel='linear')</td>\n",
       "      <td>linear</td>\n",
       "      <td>linearsvc_100.model</td>\n",
       "      <td>linear (c=100)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.1</td>\n",
       "      <td>SVC(C=0.1)</td>\n",
       "      <td>rbf</td>\n",
       "      <td>rbfsvc_0.1.model</td>\n",
       "      <td>rbf (c=0.1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.5</td>\n",
       "      <td>SVC(C=0.5)</td>\n",
       "      <td>rbf</td>\n",
       "      <td>rbfsvc_0.5.model</td>\n",
       "      <td>rbf (c=0.5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>SVC(C=1)</td>\n",
       "      <td>rbf</td>\n",
       "      <td>rbfsvc_1.model</td>\n",
       "      <td>rbf (c=1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5.0</td>\n",
       "      <td>SVC(C=5)</td>\n",
       "      <td>rbf</td>\n",
       "      <td>rbfsvc_5.model</td>\n",
       "      <td>rbf (c=5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10.0</td>\n",
       "      <td>SVC(C=10)</td>\n",
       "      <td>rbf</td>\n",
       "      <td>rbfsvc_10.model</td>\n",
       "      <td>rbf (c=10)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>100.0</td>\n",
       "      <td>SVC(C=100)</td>\n",
       "      <td>rbf</td>\n",
       "      <td>rbfsvc_100.model</td>\n",
       "      <td>rbf (c=100)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.1</td>\n",
       "      <td>SVC(C=0.1, kernel='poly')</td>\n",
       "      <td>poly</td>\n",
       "      <td>polysvc_0.1.model</td>\n",
       "      <td>poly (c=0.1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.5</td>\n",
       "      <td>SVC(C=0.5, kernel='poly')</td>\n",
       "      <td>poly</td>\n",
       "      <td>polysvc_0.5.model</td>\n",
       "      <td>poly (c=0.5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.0</td>\n",
       "      <td>SVC(C=1, kernel='poly')</td>\n",
       "      <td>poly</td>\n",
       "      <td>polysvc_1.model</td>\n",
       "      <td>poly (c=1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5.0</td>\n",
       "      <td>SVC(C=5, kernel='poly')</td>\n",
       "      <td>poly</td>\n",
       "      <td>polysvc_5.model</td>\n",
       "      <td>poly (c=5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>10.0</td>\n",
       "      <td>SVC(C=10, kernel='poly')</td>\n",
       "      <td>poly</td>\n",
       "      <td>polysvc_10.model</td>\n",
       "      <td>poly (c=10)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>100.0</td>\n",
       "      <td>SVC(C=100, kernel='poly')</td>\n",
       "      <td>poly</td>\n",
       "      <td>polysvc_100.model</td>\n",
       "      <td>poly (c=100)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        c                   classifier  kernel           model_file  \\\n",
       "0     0.1  SVC(C=0.1, kernel='linear')  linear  linearsvc_0.1.model   \n",
       "1     0.5  SVC(C=0.5, kernel='linear')  linear  linearsvc_0.5.model   \n",
       "2     1.0    SVC(C=1, kernel='linear')  linear    linearsvc_1.model   \n",
       "3     5.0    SVC(C=5, kernel='linear')  linear    linearsvc_5.model   \n",
       "4    10.0   SVC(C=10, kernel='linear')  linear   linearsvc_10.model   \n",
       "5   100.0  SVC(C=100, kernel='linear')  linear  linearsvc_100.model   \n",
       "6     0.1                   SVC(C=0.1)     rbf     rbfsvc_0.1.model   \n",
       "7     0.5                   SVC(C=0.5)     rbf     rbfsvc_0.5.model   \n",
       "8     1.0                     SVC(C=1)     rbf       rbfsvc_1.model   \n",
       "9     5.0                     SVC(C=5)     rbf       rbfsvc_5.model   \n",
       "10   10.0                    SVC(C=10)     rbf      rbfsvc_10.model   \n",
       "11  100.0                   SVC(C=100)     rbf     rbfsvc_100.model   \n",
       "12    0.1    SVC(C=0.1, kernel='poly')    poly    polysvc_0.1.model   \n",
       "13    0.5    SVC(C=0.5, kernel='poly')    poly    polysvc_0.5.model   \n",
       "14    1.0      SVC(C=1, kernel='poly')    poly      polysvc_1.model   \n",
       "15    5.0      SVC(C=5, kernel='poly')    poly      polysvc_5.model   \n",
       "16   10.0     SVC(C=10, kernel='poly')    poly     polysvc_10.model   \n",
       "17  100.0    SVC(C=100, kernel='poly')    poly    polysvc_100.model   \n",
       "\n",
       "       model_label  \n",
       "0   linear (c=0.1)  \n",
       "1   linear (c=0.5)  \n",
       "2     linear (c=1)  \n",
       "3     linear (c=5)  \n",
       "4    linear (c=10)  \n",
       "5   linear (c=100)  \n",
       "6      rbf (c=0.1)  \n",
       "7      rbf (c=0.5)  \n",
       "8        rbf (c=1)  \n",
       "9        rbf (c=5)  \n",
       "10      rbf (c=10)  \n",
       "11     rbf (c=100)  \n",
       "12    poly (c=0.1)  \n",
       "13    poly (c=0.5)  \n",
       "14      poly (c=1)  \n",
       "15      poly (c=5)  \n",
       "16     poly (c=10)  \n",
       "17    poly (c=100)  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [4:03:15<00:00, 810.87s/it]\n"
     ]
    }
   ],
   "source": [
    "#computing the predictions of each model\n",
    "results_df['predicted_y'] = results_df['classifier'].progress_apply(lambda classif: [classif.predict(x_test)])\n",
    "\n",
    "#this cell can take several minutes to run,\n",
    "#the results are thus dumped on a file \n",
    "#to access them in a different session without the need of \n",
    "#rerunning this cell\n",
    "with open('results_df.dump', 'wb') as f:\n",
    "            pickle.dump(results_df.drop('classifier', 1), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading previously computed results from file\n",
    "with open('results_df.dump', 'rb') as f:\n",
    "    try:\n",
    "        results_df = pickle.load(f)\n",
    "    except EOFError:\n",
    "        print('EOFerror')\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cells above retrieved the trained models and computed the predictions on the test set. \n",
    "\n",
    "The following cells compute the confusion matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:00<00:00, 144.68it/s]\n",
      "100%|██████████| 18/18 [00:00<00:00, 197.94it/s]\n",
      "100%|██████████| 18/18 [00:00<00:00, 213.90it/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#computing confusion matrix for each model\n",
    "results_df['cmatrix'] = results_df['predicted_y'].progress_apply(lambda y_pred: confusion_matrix(y_test, y_pred[0]))\n",
    "\n",
    "#computing, for each model, the confusion matrix \n",
    "#with each element normalized with the number of elements \n",
    "#belonging to the same true class\n",
    "results_df['cmatrix_truen'] = results_df['predicted_y'].progress_apply(lambda y_pred: confusion_matrix(y_test, y_pred[0], normalize='true'))\n",
    "\n",
    "#computing, for each model, the confusion matrix \n",
    "#with each element normalized with the number of elements \n",
    "#belonging to the same predicted class\n",
    "results_df['cmatrix_predn'] = results_df['predicted_y'].progress_apply(lambda y_pred: confusion_matrix(y_test, y_pred[0], normalize='pred'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the confusion matrices\n",
    "from matplotlib import pyplot as plt\n",
    "fig, axs = plt.subplots(3,6, figsize=(15, 8), constrained_layout=True,\n",
    "                        sharex=True, sharey=True)\n",
    "\n",
    "c_tried = [0.1, 0.5, 1, 5, 10,100]\n",
    "k_tried = ['linear', 'poly','rbf']\n",
    "\n",
    "for i_k in range(0,3):\n",
    "    for i_c in range(0,6):\n",
    "        cmat = results_df[(results_df.c == c_tried[i_c]) & (results_df.kernel == k_tried[i_k])]['cmatrix'].iloc[0]\n",
    "        im = axs[i_k][i_c].imshow(cmat, cmap='Blues')\n",
    "        axs[i_k][i_c].set_title(f\"kernel: {k_tried[i_k]}, C={c_tried[i_c]}\")\n",
    "        axs[i_k][i_c].set_xticks(range(0,22))\n",
    "        axs[i_k][i_c].set_yticks(range(0,22))\n",
    "        axs[i_k][i_c].tick_params(axis='both', which='major', labelsize=7)\n",
    "\n",
    "fig.suptitle('Unnormalized confusion matrices', fontsize=20)\n",
    "fig.supxlabel('predicted class')\n",
    "fig.supylabel('actual class')\n",
    "#cbar_ax = fig.add_axes( ax=axes.ravel().tolist())\n",
    "fig.colorbar(im,  ax=axs.ravel().tolist())\n",
    "fig.text(1.05, 0.92, \"Class labels and frequency\\n in the test set\", fontsize=15)\n",
    "#generate class legend\n",
    "class_labels = u.get_class_labels()\n",
    "class_labels['test_perc'] = class_labels['n_test']*100/class_labels['n_test'].sum()\n",
    "classes=\"\"\n",
    "perc=\"\"\n",
    "for i, row in class_labels.iterrows():\n",
    "    classes = classes + f\"{row['class_id']:>2}:  {row['class'].replace('_', ' '):<25}\\n\"\n",
    "    perc = perc + f\"{row['test_perc']:>5.2f}%\\n\"\n",
    "\n",
    "fig.text(1.05, 0.30, classes, fontsize=12)\n",
    "fig.text(1.3, 0.30, perc, fontsize=12) \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"images/conf1svm.png\" width=\"100%\">\n",
    "</center>\n",
    "\n",
    "The figure above shows the confusion matrices of the 18 models trained. The confusion matrix of a perfect model that predicts precisely each element has null off-diagonal values. Looking at this figure it is possible to do a first qualitative evaluation of the models' performances: for example the model with a rbf kernel and C=0.1 (lower left angle) appears to make significant errors identyfing elements of several different classes as \"Blips\". \n",
    "\n",
    "However, little information can be obtained from the other images and even some diagonal elements seem to have values of the same magnitude of off-diagonal ones.\n",
    "The reason behind this is that the test set, just as the training set, is heavily unbalanced. \n",
    "This characteristic will have to be taken into account during the discussion of some evaluation metrics. Starting from the confusion matrix, more explanatory images are obtained plotting the normalized confusion matrices. In the images below each element is normalized with the total number of elements belonging to the same true class. The other type of normalization computed and discussed above does not provide significantly different results, at least from a qualitative point of view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3,6, figsize=(15, 8), constrained_layout=True,\n",
    "                        sharex=True, sharey=True)\n",
    "\n",
    "for i_k in range(0,3):\n",
    "    for i_c in range(0,6):\n",
    "        cmat = results_df[(results_df.c == c_tried[i_c]) & (results_df.kernel == k_tried[i_k])]['cmatrix_truen'].iloc[0]\n",
    "        im = axs[i_k][i_c].imshow(cmat, cmap='Blues')\n",
    "        axs[i_k][i_c].set_title(f\"kernel: {k_tried[i_k]}, C={c_tried[i_c]}\")\n",
    "        axs[i_k][i_c].set_xticks(range(0,22))\n",
    "        axs[i_k][i_c].set_yticks(range(0,22))\n",
    "        axs[i_k][i_c].tick_params(axis='both', which='major', labelsize=7)\n",
    "\n",
    "fig.suptitle('Confusion matrices normalized with true number of elements in each class', fontsize=20)\n",
    "fig.supxlabel('predicted class')\n",
    "fig.supylabel('actual class')\n",
    "#cbar_ax = fig.add_axes( ax=axes.ravel().tolist())\n",
    "fig.colorbar(im,  ax=axs.ravel().tolist())\n",
    "fig.text(1.05, 0.92, \"Class labels and frequency\\n in the test set\", fontsize=15)\n",
    "#generate class legend\n",
    "class_labels = u.get_class_labels()\n",
    "class_labels['test_perc'] = class_labels['n_test']*100/class_labels['n_test'].sum()\n",
    "fig.text(1.05, 0.30, classes, fontsize=12)\n",
    "fig.text(1.3, 0.30, perc, fontsize=12) \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"images/conf2svm.png\" width=\"100%\">\n",
    "</center>\n",
    "\n",
    "In this figure showing normalized confusion matrices, it is possible to better appreciate classification errors made by the different models, and have a first qualitative idea of which hyperparameters can provide the better results. \n",
    "From this picture, all kernels seem to perform better with higher C values.\n",
    "\n",
    "Furthermore, this figure shows how most models fail in their predictions in similar ways. For example, several elements in the \"Paired Doves\" class are misclassified as \"Extremely Loud\" in most of the models. \n",
    "\n",
    "Until now I have provided only a qualitative comparison of the models. A detailed, quantitative comparison will now be carried out computing different evaluation metrics.\n",
    "These metrics try to summarize all the information contained in the confusion matrix in a single value indicative, in different ways, of the model performance.\n",
    "\n",
    "To introduce these metrics let first consider the binary classification problem. In this context, two basic metrics are defined which are then used as building blocks for most of the others: precision and recall.\n",
    "\n",
    "Precision is defined as \n",
    "\n",
    "$$\n",
    "    \\text{Precision} = \\frac{TP}{TP+FP}\n",
    "$$\n",
    "\n",
    "thus measuring the fraction of elements actual positive elements among the ones classified, by the model, as postive (TP stands for True Positive while FP stands for False Positive). Note that false negatives do not appear in the definition of precision. This metric is thus providing an indication of how much we can trust the model when it predicts an element to be positive.\n",
    "\n",
    "Recall is instead defined as\n",
    "\n",
    "$$\n",
    "\\text{Recall} = \\frac{TP}{TP+FN}\n",
    "$$\n",
    "\n",
    "thus measuring the fraction of elements correctly classified in the positive class. This metric is providing a measure of the model's ability to find all the positive elements.\n",
    "\n",
    "For multi class classification tasks, the definitions above need to be changed and new metrics are defined (Grandini et al., 2020).\n",
    "The first metric considered is accuracy which captures the same concept of recall measuring the fraction of correctly predicted elements over the total. It is defined, being $C_{ij}$ the elements of the confusion matrix, as\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{\\text{correctly predicted elements}}{\\text{total elements}} = \\frac{\\sum_i C_{ii}}{\\sum_{i,j} C_{ij}}\n",
    "$$\n",
    "\n",
    "This metric however is not well suited for unbalanced datasets because less populated classes would have a minor impact on its value. For this reason, balanced accuracy is introduced.\n",
    "Its definition is the following:\n",
    "\n",
    "$$\n",
    "\\text{Balanced accuracy} = \\frac{1}{n_{classes}} \\cdot \\sum_i \\frac{C_{ii}}{\\sum_j{C_{ij}}}\n",
    "$$\n",
    "\n",
    "which is the arithmetic mean of the recall metric computed separately for each class.\n",
    "\n",
    "The following table and image show the values of these metrics for the 18 trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:00<00:00, 12974.30it/s]\n",
      "100%|██████████| 18/18 [00:00<00:00, 7078.99it/s]\n"
     ]
    }
   ],
   "source": [
    "#Accuracy and  balanced accuracy\n",
    "def compute_balacc(cmat):\n",
    "    bacc = 0\n",
    "    for i, row in enumerate(cmat):\n",
    "        bacc += row[i]/row.sum()\n",
    "    return bacc/len(cmat)\n",
    "results_df['accuracy'] = results_df['cmatrix'].progress_apply(lambda cmat: cmat.trace()/cmat.sum())\n",
    "results_df['balanced_accuracy'] = results_df['cmatrix'].progress_apply(lambda cmat: compute_balacc(cmat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>balanced_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVC(C=0.1, kernel='linear')</td>\n",
       "      <td>0.931144</td>\n",
       "      <td>0.850016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVC(C=0.5, kernel='linear')</td>\n",
       "      <td>0.929873</td>\n",
       "      <td>0.849278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVC(C=1, kernel='linear')</td>\n",
       "      <td>0.924364</td>\n",
       "      <td>0.845418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SVC(C=5, kernel='linear')</td>\n",
       "      <td>0.922881</td>\n",
       "      <td>0.851395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SVC(C=10, kernel='linear')</td>\n",
       "      <td>0.922246</td>\n",
       "      <td>0.848367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SVC(C=100, kernel='linear')</td>\n",
       "      <td>0.920551</td>\n",
       "      <td>0.843932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SVC(C=0.1)</td>\n",
       "      <td>0.736653</td>\n",
       "      <td>0.451225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SVC(C=0.5)</td>\n",
       "      <td>0.876483</td>\n",
       "      <td>0.690136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SVC(C=1)</td>\n",
       "      <td>0.894492</td>\n",
       "      <td>0.729066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SVC(C=5)</td>\n",
       "      <td>0.932203</td>\n",
       "      <td>0.851311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>SVC(C=10)</td>\n",
       "      <td>0.937076</td>\n",
       "      <td>0.859667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>SVC(C=100)</td>\n",
       "      <td>0.941949</td>\n",
       "      <td>0.866513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>SVC(C=0.1, kernel='poly')</td>\n",
       "      <td>0.897458</td>\n",
       "      <td>0.750632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>SVC(C=0.5, kernel='poly')</td>\n",
       "      <td>0.929661</td>\n",
       "      <td>0.851862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>SVC(C=1, kernel='poly')</td>\n",
       "      <td>0.934322</td>\n",
       "      <td>0.857298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>SVC(C=5, kernel='poly')</td>\n",
       "      <td>0.936864</td>\n",
       "      <td>0.862056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>SVC(C=10, kernel='poly')</td>\n",
       "      <td>0.933051</td>\n",
       "      <td>0.856226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>SVC(C=100, kernel='poly')</td>\n",
       "      <td>0.930085</td>\n",
       "      <td>0.860226</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     classifier  accuracy  balanced_accuracy\n",
       "0   SVC(C=0.1, kernel='linear')  0.931144           0.850016\n",
       "1   SVC(C=0.5, kernel='linear')  0.929873           0.849278\n",
       "2     SVC(C=1, kernel='linear')  0.924364           0.845418\n",
       "3     SVC(C=5, kernel='linear')  0.922881           0.851395\n",
       "4    SVC(C=10, kernel='linear')  0.922246           0.848367\n",
       "5   SVC(C=100, kernel='linear')  0.920551           0.843932\n",
       "6                    SVC(C=0.1)  0.736653           0.451225\n",
       "7                    SVC(C=0.5)  0.876483           0.690136\n",
       "8                      SVC(C=1)  0.894492           0.729066\n",
       "9                      SVC(C=5)  0.932203           0.851311\n",
       "10                    SVC(C=10)  0.937076           0.859667\n",
       "11                   SVC(C=100)  0.941949           0.866513\n",
       "12    SVC(C=0.1, kernel='poly')  0.897458           0.750632\n",
       "13    SVC(C=0.5, kernel='poly')  0.929661           0.851862\n",
       "14      SVC(C=1, kernel='poly')  0.934322           0.857298\n",
       "15      SVC(C=5, kernel='poly')  0.936864           0.862056\n",
       "16     SVC(C=10, kernel='poly')  0.933051           0.856226\n",
       "17    SVC(C=100, kernel='poly')  0.930085           0.860226"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df[['classifier', 'accuracy', 'balanced_accuracy']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2, figsize=(10, 5), constrained_layout=True,\n",
    "                        sharex=True, sharey=True)\n",
    "for k in results_df['kernel'].unique():\n",
    "    axes[0].plot(results_df[results_df.kernel==k]['c'], results_df[results_df.kernel==k]['accuracy'], marker='o', linestyle='--', label=k)\n",
    "    axes[1].plot(results_df[results_df.kernel==k]['c'], results_df[results_df.kernel==k]['balanced_accuracy'], marker='o', linestyle='--', label=k)\n",
    "\n",
    "axes[0].set_title('Accuracy')\n",
    "axes[1].set_title('Balanced accuracy')\n",
    "axes[0].set_xscale('log')\n",
    "axes[0].set_xlabel('C')\n",
    "axes[1].set_xlabel('C')\n",
    "axes[0].legend(loc='lower right')\n",
    "axes[1].legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"images/accsvm.png\" width=\"50%\">\n",
    "</center>\n",
    "\n",
    "Qualitatively accuracy and balanced accuracy exhibit the same trend. With linear svc similar values are obtained indipendently to the C parameter. Polynomial and rbf kernels appear to perform better than linear svc for sufficiently high values of the regularization parameter.\n",
    "\n",
    "However, the balanced accuracy assumes significantly lower values probably due to some classification errors in less populated classes. \n",
    "These values do not reach 90%.\n",
    "\n",
    "Both the accuracy and the balanced accuracy, measure the fraction of correct predictions among the total number of cases examined.\n",
    "\n",
    "I computed two additional metrics: the F1-score and the Matthew Correlation Coefficient (Grandini, 2020).\n",
    "\n",
    "The F1-score aggregates recall and precision computing their harmonic mean. It is thus defined as:\n",
    "$$\n",
    "\\text{F1-score} = \\frac{2}{\\text{precision}^{-1} + \\text{recall}^{-1}}\n",
    "$$\n",
    "For a binary classification problem this definition does not require further explanation. However, for multi-class problems the definitions of precision and accuracy can be generalized in different ways. \n",
    "I will use the definitions used to compute the macro F1-Score which treats each class in the same way without distinction between high and low populated classes making it a suitable choice for unbalanced datasets as the one used here.\n",
    "With this choice precision and recall are defined as:\n",
    "$$\n",
    "    \\text{Precision} = \\frac 1 K \\sum_k \\frac{TP_k}{TP_k + FP_k} \\\\\n",
    "    \\text{Accuracy} = \\frac 1 K \\sum_k \\frac{TP_k}{TP_k + FN_k}\n",
    "$$\n",
    "where the index $k$ runs over the $K$ classes. These definitions are just arithmetic means of the two metrics computed separately for each class.\n",
    "F1-score takes values in the range \\[0;1\\]. The harmonic mean tends to give more weight to lower values thus causing a significant drop of the metric when the accuracy or the recall is low.\n",
    "\n",
    "The Matthew Correlation Coefficient, for multiclass classification is defined as follows:\n",
    "$$\n",
    "    MCC = \\frac{c\\cdot s - \\sum_k^K(p_k \\cdot t_k)}{\\sqrt{(s^2-\\sum_k^Kp_k^2)\\cdot(s^2 - \\sum_k^Kt_k^2)}} \\\\\n",
    "$$\n",
    "where:\n",
    "\n",
    "$c = \\sum_k^K C_{kk}$ total number of correctly predicted elements\n",
    "\n",
    "$s = \\sum_i^K \\sum_j^K C_{ij}$ total number of elements\n",
    "\n",
    "$p_k = \\sum_i^K C_{ki}$ number of times that the class k was predicted\n",
    "\n",
    "$t_k = \\sum_i^K C_{ik}$ number of elements truly of the class k\n",
    "\n",
    "MCC is, in essence, a correlation coefficient between the correct and predicted labels. It takes values in the range \\[-1;1\\] where 1 indicates a perfect prediction, 0 an average random prediction while -1 indicates an inverse prediction. In the latter case it is usually possible to correct some errors in the implementation to obtain good predictions. This coefficients takes into account the entire confusion matrix and is usually considered a measure with balanced contributions from each class which makes it suited for unbalanced datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#F1 score\n",
    "def compute_macroF1(cmat):\n",
    "    mprec = 0\n",
    "    mrec = 0\n",
    "    #compute macro precision and macro recall\n",
    "    for i in range(0, len(cmat)):\n",
    "        mprec += (cmat[i][i]/(cmat[:,i].sum()+1e-20))\n",
    "        mrec  += (cmat[i][i]/cmat[i].sum())\n",
    "   \n",
    "    mprec /= len(cmat)\n",
    "    mrec  /= len(cmat)\n",
    "\n",
    "    return 2*(mprec*mrec)/(mprec+mrec)\n",
    "\n",
    "def compute_MCC(cmat):\n",
    "    c = cmat.trace()\n",
    "    s = cmat.sum()\n",
    "    p = [cmat[:,k].sum()+1e-20 for k in range(0, len(cmat))]\n",
    "    t = [cmat[k].sum() for k in range(0, len(cmat))]\n",
    "\n",
    "    mcc = (c*s - np.dot(p,t))/np.sqrt((s**2-np.dot(p,p))*(s**2-np.dot(t,t)))\n",
    "    return mcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:00<00:00, 2728.99it/s]\n",
      "100%|██████████| 18/18 [00:00<00:00, 3440.30it/s]\n"
     ]
    }
   ],
   "source": [
    "results_df['macroF1'] = results_df['cmatrix'].progress_apply(lambda cmat: compute_macroF1(cmat))\n",
    "results_df['MCC'] = results_df['cmatrix'].progress_apply(lambda cmat: compute_MCC(cmat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following table and figure show the results obtained computing these metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>macroF1</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVC(C=0.1, kernel='linear')</td>\n",
       "      <td>0.874281</td>\n",
       "      <td>0.923925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVC(C=0.5, kernel='linear')</td>\n",
       "      <td>0.872547</td>\n",
       "      <td>0.922510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVC(C=1, kernel='linear')</td>\n",
       "      <td>0.867138</td>\n",
       "      <td>0.916435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SVC(C=5, kernel='linear')</td>\n",
       "      <td>0.869680</td>\n",
       "      <td>0.914818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SVC(C=10, kernel='linear')</td>\n",
       "      <td>0.864445</td>\n",
       "      <td>0.914115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SVC(C=100, kernel='linear')</td>\n",
       "      <td>0.860666</td>\n",
       "      <td>0.912240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SVC(C=0.1)</td>\n",
       "      <td>0.542700</td>\n",
       "      <td>0.711926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SVC(C=0.5)</td>\n",
       "      <td>0.722294</td>\n",
       "      <td>0.864479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SVC(C=1)</td>\n",
       "      <td>0.782393</td>\n",
       "      <td>0.884236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SVC(C=5)</td>\n",
       "      <td>0.875417</td>\n",
       "      <td>0.925102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>SVC(C=10)</td>\n",
       "      <td>0.883033</td>\n",
       "      <td>0.930476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>SVC(C=100)</td>\n",
       "      <td>0.888522</td>\n",
       "      <td>0.935838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>SVC(C=0.1, kernel='poly')</td>\n",
       "      <td>0.809315</td>\n",
       "      <td>0.887179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>SVC(C=0.5, kernel='poly')</td>\n",
       "      <td>0.875688</td>\n",
       "      <td>0.922300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>SVC(C=1, kernel='poly')</td>\n",
       "      <td>0.881102</td>\n",
       "      <td>0.927445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>SVC(C=5, kernel='poly')</td>\n",
       "      <td>0.884240</td>\n",
       "      <td>0.930231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>SVC(C=10, kernel='poly')</td>\n",
       "      <td>0.877370</td>\n",
       "      <td>0.926025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>SVC(C=100, kernel='poly')</td>\n",
       "      <td>0.879102</td>\n",
       "      <td>0.922769</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     classifier   macroF1       MCC\n",
       "0   SVC(C=0.1, kernel='linear')  0.874281  0.923925\n",
       "1   SVC(C=0.5, kernel='linear')  0.872547  0.922510\n",
       "2     SVC(C=1, kernel='linear')  0.867138  0.916435\n",
       "3     SVC(C=5, kernel='linear')  0.869680  0.914818\n",
       "4    SVC(C=10, kernel='linear')  0.864445  0.914115\n",
       "5   SVC(C=100, kernel='linear')  0.860666  0.912240\n",
       "6                    SVC(C=0.1)  0.542700  0.711926\n",
       "7                    SVC(C=0.5)  0.722294  0.864479\n",
       "8                      SVC(C=1)  0.782393  0.884236\n",
       "9                      SVC(C=5)  0.875417  0.925102\n",
       "10                    SVC(C=10)  0.883033  0.930476\n",
       "11                   SVC(C=100)  0.888522  0.935838\n",
       "12    SVC(C=0.1, kernel='poly')  0.809315  0.887179\n",
       "13    SVC(C=0.5, kernel='poly')  0.875688  0.922300\n",
       "14      SVC(C=1, kernel='poly')  0.881102  0.927445\n",
       "15      SVC(C=5, kernel='poly')  0.884240  0.930231\n",
       "16     SVC(C=10, kernel='poly')  0.877370  0.926025\n",
       "17    SVC(C=100, kernel='poly')  0.879102  0.922769"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df[['classifier', 'macroF1', 'MCC']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2, figsize=(10, 5), constrained_layout=True,\n",
    "                        sharex=True, sharey=True)\n",
    "for k in results_df['kernel'].unique():\n",
    "    axes[0].plot(results_df[results_df.kernel==k]['c'], results_df[results_df.kernel==k]['macroF1'], marker='o', linestyle='--', label=k)\n",
    "    axes[1].plot(results_df[results_df.kernel==k]['c'], results_df[results_df.kernel==k]['MCC'], marker='o', linestyle='--', label=k)\n",
    "\n",
    "axes[0].set_title('Macro F1 score')\n",
    "axes[1].set_title('MCC')\n",
    "axes[0].set_xscale('log')\n",
    "axes[0].set_xlabel('C')\n",
    "axes[1].set_xlabel('C')\n",
    "axes[0].legend(loc='lower right')\n",
    "axes[1].legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"images/f1svm.png\" width=\"50%\">\n",
    "</center>\n",
    "\n",
    "In this case these new metrics do not seem to provide new insights on the behaviour of the trained models. The same observations made discussing the accuracy are still valid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I have applied a support vector machine to the gravity spy dataset exploring different values for the two main hyperparameters that characterize this machine learning method.\n",
    "\n",
    "Among the 3 kernels tried, the linear one appears, generally, to be the worst even if it performs better than the others for the lowest C value.\n",
    "\n",
    "The best accuracy of 94.19% (and balanced accuracy of 86.65%) were obtained with the rbf kernel using C=100.\n",
    "\n",
    "The scores obtained with the polynomial kernel and C=5, are the second best ones: accuracy=93.69%, balanced accuracy=86.21%.\n",
    "\n",
    "However, these two model require significatively different times when applied to new data. In the following cells I have used the %%timeit directive to measure the time needed to classify 100 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 s ± 2.22 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "#opening best SVM classifier\n",
    "with open('polysvc_5.model', 'rb') as f:\n",
    "        classifier = pickle.load(f)\n",
    "        classifier.predict(x_test[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.5 s ± 2.01 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "#opening best SVM classifier\n",
    "with open('rbfsvc_100.model', 'rb') as f:\n",
    "        classifier = pickle.load(f)\n",
    "        classifier.predict(x_test[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results clearly show that the model using the polynomial kernel is faster. Since the scores obtained are very similar I have selected it as the best SVM classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Géron, Aurélien. _Hands-on machine learning with Scikit-Learn, Keras, and TensorFlow: Concepts, tools, and techniques to build intelligent systems._ O'Reilly Media, 2019.\n",
    "\n",
    "Grandini, Margherita, Enrico Bagli, and Giorgio Visani. _Metrics for multi-class classification: an overview._ arXiv preprint arXiv:2008.05756 (2020).\n",
    "\n",
    "scikit-learn developers, _sklearn.svm.SVC — scikit-learn 1.0.2 documentation_, Available at: https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC\n",
    "\n",
    "scikit-learn developers, _Support Vector Machines  — scikit-learn 1.0.2 documentation_, Available at: https://scikit-learn.org/stable/modules/svm.html (Accessed: 01/2022)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python39564bit80cf730d14af42a98fb2c59e9ca33086"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
